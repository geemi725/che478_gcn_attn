{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.optimizers as optimizers\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import (Conv, Dense, MaxPool, Relu, Flatten)\n",
    "from jax import jit, grad, random,vmap,value_and_grad\n",
    "import jax.nn as jnn\n",
    "from jax.tree_util import tree_multimap\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from functools import partial # for use with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parse(record):\n",
    "    features = {\n",
    "        'N': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'labels': tf.io.FixedLenFeature([16], tf.float32),\n",
    "        'elements': tf.io.VarLenFeature(tf.int64),\n",
    "        'coords': tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "        serialized=record, features=features)\n",
    "    coords = tf.reshape(tf.sparse.to_dense(parsed_features['coords'], default_value=0),[-1,4])\n",
    "    elements = tf.sparse.to_dense(parsed_features['elements'], default_value=0)\n",
    "    return (elements, coords), parsed_features['labels']\n",
    "data = tf.data.TFRecordDataset(\n",
    "    'qm9.tfrecords', compression_type='GZIP').map(data_parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(e,x):\n",
    "    e = e.numpy()\n",
    "    x = x.numpy()\n",
    "    r = x[:,:3]\n",
    "    r2 = np.sum((r - r[:,np.newaxis,:])**2,axis=-1)\n",
    "    edges = np.where(r2!=0, 1/r2,0.0) #[N,N]\n",
    "    nodes = np.zeros((len(e),9))\n",
    "    nodes[np.arange(len(e)), e-1] = 1\n",
    "    return nodes,edges\n",
    "\n",
    "def get_label(y):\n",
    "    return y.numpy()[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of GCN with attention\n",
    "\n",
    "def GCN(out_dim,embed_dim):\n",
    "    def init_fun(global_ft_len):\n",
    "        #output_shape = input_shape[:-1] + (out_dim,)\n",
    "        #trainable weights\n",
    "        #w = np.random.normal(size =(4, embed_dim,out_dim), scale=1e-1)\n",
    "        wq = np.random.normal(size =(embed_dim,out_dim), scale=1e-1)\n",
    "        wk = np.random.normal(size =(embed_dim,out_dim), scale=1e-1)\n",
    "        wv = np.random.normal(size =(embed_dim,out_dim), scale=1e-1)\n",
    "        wn = np.random.normal(size =(embed_dim,out_dim), scale=1e-1)\n",
    "        wu = np.random.normal(size =(out_dim,global_ft_len), scale=1e-1)\n",
    "    \n",
    "        return (wq,wk,wv,wn,wu)\n",
    "    \n",
    "    def apply_fun(train_weights,nodes,edges,features, **kwargs):\n",
    "       \n",
    "        query = jnp.dot(nodes,train_weights[0]) \n",
    "        \n",
    "        keys = jnp.dot(jnp.repeat(nodes[jnp.newaxis,...],nodes.shape[0],axis=0), \n",
    "                       train_weights[1])* edges[...,jnp.newaxis]\n",
    "        \n",
    "        d_sq = math.sqrt(keys.shape[-1])\n",
    "        b = jnn.softmax(query[jnp.newaxis,...] * keys/d_sq)\n",
    "       \n",
    "        values = jnp.dot(jnp.repeat(nodes[jnp.newaxis,...],nodes.shape[0],axis=0), train_weights[2])\n",
    "\n",
    "        messages = b * values \n",
    "        \n",
    "        net_message = jnp.mean(messages,axis= 1)\n",
    "        \n",
    "        self_message = jnp.dot(nodes, train_weights[3])\n",
    "\n",
    "        #self loop\n",
    "        out_nodes = jnn.relu((net_message))+self_message\n",
    "        \n",
    "        #global features\n",
    "        global_node_features = jnp.sum(out_nodes, axis=0)\n",
    "       \n",
    "        new_features = jax.nn.relu(global_node_features@train_weights[4]) + features\n",
    "\n",
    "        return out_nodes,edges,new_features\n",
    "    \n",
    "    return init_fun,apply_fun\n",
    "  \n",
    "\n",
    "def y_hat(n,e,params1,params2,params3,b):\n",
    "    init_fts = jnp.zeros(global_ft_len)\n",
    "    n,e,fts = gcn_apply(params1,n,e,init_fts)\n",
    "    n,e,fts = gcn_apply(params2,n,e,fts)\n",
    "    y_hat = fts @ params3 + b\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss(nodes,edges, targets,params1,params2,params3,b):\n",
    "    predictions = y_hat(nodes,edges,params1,params2,params3,b)\n",
    "    \n",
    "    return (targets - predictions)**2\n",
    "\n",
    "#gradient of loss wrt params1,params2,params3,b\n",
    "loss_grad = jax.grad(loss, (3, 4, 5,6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "out_dim = 9\n",
    "embed_dim = 9\n",
    "global_ft_len = 8\n",
    "\n",
    "#You have to plot for different datasets\n",
    "#Plot the test-loss for test set\n",
    "\n",
    "test_set = data.take(100)\n",
    "valid_test_len = 100\n",
    "valid_set = data.skip(100).take(10)\n",
    "train_set = data.skip(110).take(50).shuffle(50)\n",
    "\n",
    "gcn_init,gcn_apply = GCN(out_dim,embed_dim)\n",
    "params1 = gcn_init(global_ft_len)\n",
    "params2 = gcn_init(global_ft_len)\n",
    "params3 = np.random.normal(size=(global_ft_len))\n",
    "\n",
    "b = 245.\n",
    "\n",
    "\n",
    "epochs = 16\n",
    "batch_size = 32\n",
    "eta = 1e-2\n",
    "val_loss = [0. for _ in range(epochs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gwellawa/.conda/htf2/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 923.17\n",
      "epoch: 1 loss: 922.10\n",
      "epoch: 2 loss: 921.03\n",
      "epoch: 3 loss: 919.95\n",
      "epoch: 4 loss: 918.90\n",
      "epoch: 5 loss: 917.85\n",
      "epoch: 6 loss: 916.76\n",
      "epoch: 7 loss: 915.69\n",
      "epoch: 8 loss: 914.65\n",
      "epoch: 9 loss: 913.58\n",
      "epoch: 10 loss: 912.53\n",
      "epoch: 11 loss: 911.44\n",
      "epoch: 12 loss: 910.39\n",
      "epoch: 13 loss: 909.34\n",
      "epoch: 14 loss: 452.00\n",
      "epoch: 15 loss: 451.70\n"
     ]
    }
   ],
   "source": [
    "#Now this is training.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bi = 0\n",
    "    grad_est = None\n",
    "    for d in train_set:         \n",
    "        # do training step\n",
    "        # but do not update\n",
    "        # until have enough points\n",
    "        (e,x), y = d\n",
    "        nodes,edges = make_graph(e,x)\n",
    "        label = get_label(y)\n",
    "        \n",
    "        if grad_est is None:\n",
    "            grad_est = loss_grad(nodes,edges, label, params1, params2, params3, b)\n",
    "        else:\n",
    "            grad_est += loss_grad(nodes,edges, label, params1, params2, params3, b)\n",
    "        bi += 1\n",
    "        if bi == batch_size:\n",
    "            # have enough to update            \n",
    "            # update regression weights\n",
    "            params3 -= eta * grad_est[2]  / batch_size\n",
    "            b -= eta * grad_est[3]  / batch_size\n",
    "            # update GNN weights            \n",
    "            for i,w in [(0, params1), (1, params2)]:\n",
    "                for j, param in enumerate(w):\n",
    "                    param -= eta * grad_est[i][j] / batch_size\n",
    "            # reset tracking of batch index\n",
    "            bi = 0            \n",
    "            grad_est = None            \n",
    "    # compute validation loss    \n",
    "    for v in valid_set:\n",
    "        (e,x), y = v\n",
    "        nodes,edges = make_graph(e,x)\n",
    "        label = get_label(y)\n",
    "        # convert SE to RMSE\n",
    "        val_loss[epoch] += jnp.sqrt(loss(nodes,edges, label, params1, params2, params3, b) / valid_test_len)\n",
    "    print('epoch:', epoch, 'loss: {:.2f}'.format(val_loss[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the following cell, used to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 9) embed nodes\n",
      "input feautres [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "output features (8,)\n",
      "221.61868\n",
      "525.4042\n"
     ]
    }
   ],
   "source": [
    "features = jnp.zeros(global_ft_len)\n",
    "print(nodes.shape,'embed nodes')\n",
    "\n",
    "out = gcn_apply(gcn_init(global_ft_len),nodes,edges,features)\n",
    "\n",
    "print('input feautres', features)\n",
    "print('output features', out[2].shape)\n",
    "pred = y_hat(nodes,edges, gcn_init(global_ft_len), gcn_init(global_ft_len), np.random.normal(size=(global_ft_len)), 222.)\n",
    "print(pred)\n",
    "grad_est = loss_grad(nodes,edges, label, gcn_init(global_ft_len), gcn_init(global_ft_len)\n",
    "                     , np.random.normal(size=(global_ft_len)), 222.)\n",
    "print(grad_est[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (htff2)",
   "language": "python",
   "name": "htf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
